version: '3.8'
services:
  spark:
    image: bitnami/spark:3.5
    hostname: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_MASTER_HOST=0.0.0.0
      - SPARK_MASTER_WEBUI_PORT=8081
      - PYSPARK_PYTHON=python3
    ports:
      - "7077:7077"
      - "8081:8081"
    volumes:
      - spark-data:/opt/bitnami/spark/data
      - ./spark-apps:/opt/bitnami/spark/apps
    networks:
      - spark-network
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
      placement:
        constraints:
          - node.role == manager

  spark-worker:
    image: bitnami/spark:3.5
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark:7077
      - SPARK_WORKER_WEBUI_PORT=8082
      - PYSPARK_PYTHON=python3
    ports:
      - "8082:8082"
    depends_on:
      - spark
    networks:
      - spark-network
    deploy:
      replicas: 2
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    command: redis-server --save 60 1 --loglevel warning
    networks:
      - spark-network
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
        
  postgres:
    image: postgres:15-alpine
    ports:
      - "15432:15432"
    environment:
      - POSTGRES_DB=dataframe_ui
      - POSTGRES_USER=dataframe_user
      - POSTGRES_PASSWORD=dataframe_password
      - PGPORT=15432
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./dataframe-ui-x/init.sql:/docker-entrypoint-initdb.d/init.sql
    networks:
      - spark-network
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3

  dataframe-api:
    build: ./dataframe-api
    ports:
      - "4999:4999"
    environment:
      - FLASK_ENV=production
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - PORT=4999
      # API Protection settings
      - ENABLE_API_PROTECTION=true
      - INTERNAL_API_KEY=dataframe-api-internal-key
      - ALLOWED_USER_AGENTS=python-requests,curl,wget,httpie,postman,insomnia
      # Spark client configuration for compare endpoint
      - SPARK_MASTER_URL=spark://spark:7077
      - SPARK_DRIVER_HOST=dataframe-api
      - SPARK_DRIVER_BIND_ADDRESS=0.0.0.0
      - SPARK_EXECUTOR_MEMORY=4g
      - SPARK_DRIVER_MEMORY=4g
      - PYSPARK_PYTHON=python3
      - PYSPARK_DRIVER_PYTHON=python3
      # ntfy notifications
      - NTFY_ENABLE=true
      - NTFY_URL=https://ntfy:8443
      - NTFY_TOPIC=spark
    depends_on:
      - redis
      - spark
    volumes:
      - ./dataframe-api:/app
    networks:
      - spark-network
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3

  dataframe-ui-x:
    build: ./dataframe-ui-x
    ports:
      - "5001:5001"
    environment:
      - PORT=5001
      - API_BASE_URL=http://dataframe-api:4999
      # Authentication settings
      - SECRET_KEY=dataframe-ui-secret-key-change-in-production
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=15432
      - POSTGRES_DB=dataframe_ui
      - POSTGRES_USER=dataframe_user
      - POSTGRES_PASSWORD=dataframe_password
      # UI settings
      - MAX_ITEMS_PER_PAGE=15
      - LOGIN_GRADIENT_THEME=forest
      # Chained pipeline settings
      - MAX_CHAINED_PIPELINES=10
    depends_on:
      - dataframe-api
      - postgres
    networks:
      - spark-network
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3

  ntfy:
    image: binwiederhier/ntfy
    ports:
      - "8080:8080"
    volumes:
      - ntfy-data:/var/lib/ntfy
      - ./ntfy/server.yml:/etc/ntfy/server.yml
    command: serve
    networks:
      - spark-network
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3

  nginx:
    image: nginx:alpine
    ports:
      - "8880:8880"
      - "8443:8443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf
      - ./nginx/ssl:/etc/nginx/ssl
    depends_on:
      - ntfy
    networks:
      - spark-network
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3

networks:
  spark-network:
    driver: overlay
    attachable: true

volumes:
  spark-data:
    driver: local
  redis-data:
    driver: local
  postgres-data:
    driver: local
  ntfy-data:
    driver: local