version: '3.8'
services:
  spark:
    image: bitnami/spark:3.5
    restart: always
    hostname: localhost
    network_mode: "host"
    container_name: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_MASTER_HOST=127.0.0.1
      - SPARK_MASTER_WEBUI_PORT=8081  # Set Web UI to port 8081
      # Ensure PySpark uses python3 in cluster
      - PYSPARK_PYTHON=python3
    volumes:
      - spark-data:/opt/bitnami/spark/data
      - ./spark-apps:/opt/bitnami/spark/apps  # Mount for your Spark applications
  
  spark-worker:
    image: bitnami/spark:3.5
    container_name: spark-worker
    restart: always
    network_mode: "host"
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://127.0.0.1:7077
      - SPARK_WORKER_WEBUI_PORT=8082
      # Ensure executors run python3
      - PYSPARK_PYTHON=python3
    depends_on:
      - spark
  
  redis:
    image: redis:7-alpine
    restart: always
    network_mode: "host"
    volumes:
      - redis-data:/data
    command: redis-server --save 60 1 --loglevel warning --port 6379
  
  postgres:
    image: postgres:15-alpine
    restart: always
    network_mode: "host"
    environment:
      - POSTGRES_DB=dataframe_ui
      - POSTGRES_USER=dataframe_user
      - POSTGRES_PASSWORD=dataframe_password
      - PGPORT=15432
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./dataframe-ui-x/init.sql:/docker-entrypoint-initdb.d/init.sql
  
  dataframe-api:
    build: ./dataframe-api
    container_name: dataframe-api
    restart: always
    network_mode: "host"
    environment:
      - FLASK_ENV=production
      - REDIS_HOST=localhost
      - REDIS_PORT=6379
      - PORT=4999
      # API Protection settings
      - ENABLE_API_PROTECTION=true
      - INTERNAL_API_KEY=dataframe-api-internal-key
      - ALLOWED_USER_AGENTS=python-requests,curl,wget,httpie,postman,insomnia
      # Spark client configuration for compare endpoint
      - SPARK_MASTER_URL=spark://127.0.0.1:7077
      # Optional: set to a reachable host IP if workers cannot connect back to driver
      - SPARK_DRIVER_HOST=127.0.0.1
      - SPARK_DRIVER_BIND_ADDRESS=0.0.0.0
      - SPARK_EXECUTOR_MEMORY=4g
      - SPARK_DRIVER_MEMORY=4g
      # Align driver/executor Python versions
      - PYSPARK_PYTHON=python3
      - PYSPARK_DRIVER_PYTHON=python3
      # ntfy notifications
      - NTFY_ENABLE=true
      - NTFY_URL=https://localhost:8443
      - NTFY_TOPIC=spark
    depends_on:
      - redis
    volumes:
      - ./dataframe-api:/app

  dataframe-ui-x:
    build: ./dataframe-ui-x
    container_name: dataframe-ui-x
    restart: always
    network_mode: "host"
    environment:
      - PORT=5001
      - API_BASE_URL=http://localhost:4999
      # Authentication settings
      - SECRET_KEY=dataframe-ui-secret-key-change-in-production
      - POSTGRES_HOST=localhost
      - POSTGRES_PORT=15432
      - POSTGRES_DB=dataframe_ui
      - POSTGRES_USER=dataframe_user
      - POSTGRES_PASSWORD=dataframe_password
      # UI settings
      - MAX_ITEMS_PER_PAGE=15
      - LOGIN_GRADIENT_THEME=forest
      # Chained pipeline settings
      - MAX_CHAINED_PIPELINES=10
    depends_on:
      - dataframe-api
      - postgres

  ntfy:
    image: binwiederhier/ntfy
    container_name: ntfy
    restart: always
    network_mode: "host"
    volumes:
      - ntfy-data:/var/lib/ntfy
      - ./ntfy/server.yml:/etc/ntfy/server.yml
    command: serve
  
  nginx:
    image: nginx:alpine
    container_name: nginx-ntfy
    restart: always
    network_mode: "host"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf
      - ./nginx/ssl:/etc/nginx/ssl
    depends_on:
      - ntfy

  playwright:
    image: mcr.microsoft.com/playwright:v1.40.0-focal
    container_name: playwright-tests
    working_dir: /app
    environment:
      - CI=true
      - PLAYWRIGHT_SKIP_BROWSER_DOWNLOAD=1
      - TEST_BASE_URL=http://localhost:5001
    volumes:
      - ./playwright-tests:/app
      - ./test-results:/app/test-results
      - ./playwright-report:/app/playwright-report
    network_mode: "host"
    command: ["sleep", "infinity"]
    profiles:
      - testing

volumes:
  spark-data:
    driver: local
  redis-data:
    driver: local
  postgres-data:
    driver: local
  ntfy-data:
    driver: local
