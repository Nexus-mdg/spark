name: Tests

on:
  push:
    branches: [ main, master, develop ]
  pull_request:
    branches: [ main, master, develop ]
  workflow_dispatch:

jobs:
  # Infrastructure tests (always run first)
  test-infrastructure:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.12'

    - name: Install test dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r tests/requirements.txt

    - name: Run infrastructure tests
      run: |
        chmod +x tests/scripts/run_tests.sh
        ./tests/scripts/run_tests.sh infrastructure

  # Core API tests without Spark (faster and more reliable)
  test-api-core:
    runs-on: ubuntu-latest
    needs: test-infrastructure

    services:
      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.12'

    - name: Install dependencies
      run: |
        cd dataframe-api
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        cd ..
        pip install -r tests/requirements.txt

    - name: Wait for Redis
      run: |
        until timeout 2 bash -c "</dev/tcp/localhost/6379"; do
          echo "Waiting for Redis..."
          sleep 2
        done
        echo "Redis is ready!"

    - name: Start dataframe-api service (without Spark)
      run: |
        cd dataframe-api
        export FLASK_ENV=development
        export REDIS_HOST=localhost
        export REDIS_PORT=6379
        export PORT=4999
        # Disable Spark for core tests - API will fallback to pandas
        export SPARK_MASTER_URL=""
        export ENABLE_API_PROTECTION=false
        python app.py &
        APP_PID=$!
        echo "APP_PID=$APP_PID" >> $GITHUB_ENV
        
        # Wait for the app to start
        sleep 10

    - name: Wait for API readiness
      run: |
        chmod +x tests/scripts/run_tests.sh
        export API_BASE=http://localhost:4999
        ./tests/scripts/run_tests.sh wait-api

    - name: Run modern API tests
      run: |
        export API_BASE=http://localhost:4999
        export TEST_ENV=ci
        ./tests/scripts/run_tests.sh api --coverage --html

    - name: Upload API test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: api-test-results
        path: |
          tests/reports/
          .pytest_cache/
        retention-days: 30

    - name: Cleanup
      if: always()
      run: |
        if [ ! -z "$APP_PID" ]; then
          kill $APP_PID || true
        fi

  # Visual regression tests (requires UI)
  test-visual:
    runs-on: ubuntu-latest
    needs: test-infrastructure
    
    services:
      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
      
      postgres:
        image: postgres:15
        env:
          POSTGRES_DB: dataframe_ui
          POSTGRES_USER: dataframe_user
          POSTGRES_PASSWORD: dataframe_password
          POSTGRES_HOST_AUTH_METHOD: trust
        ports:
          - 15432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.12'

    - name: Set up Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '18'

    - name: Install API dependencies
      run: |
        cd dataframe-api
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Install UI dependencies
      run: |
        cd dataframe-ui-x
        pip install -r requirements.txt

    - name: Install test dependencies
      run: |
        pip install -r tests/requirements.txt

    - name: Install Playwright
      run: |
        cd tests/visual
        npm install
        npx playwright install --with-deps chromium

    - name: Start dataframe-api service
      run: |
        cd dataframe-api
        export FLASK_ENV=development
        export REDIS_HOST=localhost
        export REDIS_PORT=6379
        export PORT=4999
        export SPARK_MASTER_URL=""
        export ENABLE_API_PROTECTION=false
        python app.py &
        API_PID=$!
        echo "API_PID=$API_PID" >> $GITHUB_ENV
        sleep 10

    - name: Start dataframe-ui-x service
      run: |
        cd dataframe-ui-x
        export PORT=5001
        export API_BASE_URL=http://localhost:4999
        export DATABASE_URL=postgresql://dataframe_user:dataframe_password@localhost:15432/dataframe_ui
        export DISABLE_AUTHENTICATION=true
        export SECRET_KEY=test-secret-key
        python app.py &
        UI_PID=$!
        echo "UI_PID=$UI_PID" >> $GITHUB_ENV
        sleep 15

    - name: Wait for services
      run: |
        chmod +x tests/scripts/run_tests.sh
        export API_BASE=http://localhost:4999
        export UI_BASE=http://localhost:5001
        ./tests/scripts/run_tests.sh wait-api
        ./tests/scripts/run_tests.sh wait-ui

    - name: Run visual tests
      run: |
        export API_BASE=http://localhost:4999
        export UI_BASE=http://localhost:5001
        export TEST_ENV=ci
        ./tests/scripts/run_tests.sh visual

    - name: Upload visual test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: visual-test-results
        path: |
          tests/visual/test-results/
          tests/visual/playwright-report/
          tests/reports/visual_reports/
        retention-days: 30

    - name: Cleanup
      if: always()
      run: |
        if [ ! -z "$API_PID" ]; then
          kill $API_PID || true
        fi
        if [ ! -z "$UI_PID" ]; then
          kill $UI_PID || true
        fi

  # Legacy shell-based tests (for compatibility)
  test-legacy:
    runs-on: ubuntu-latest
    needs: test-infrastructure

    services:
      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.12'

    - name: Install dependencies
      run: |
        cd dataframe-api
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Wait for Redis
      run: |
        until timeout 2 bash -c "</dev/tcp/localhost/6379"; do
          echo "Waiting for Redis..."
          sleep 2
        done
        echo "Redis is ready!"

    - name: Start dataframe-api service (without Spark)
      run: |
        cd dataframe-api
        export FLASK_ENV=development
        export REDIS_HOST=localhost
        export REDIS_PORT=6379
        export PORT=4999
        export SPARK_MASTER_URL=""
        export ENABLE_API_PROTECTION=false
        python app.py &
        APP_PID=$!
        echo "APP_PID=$APP_PID" >> $GITHUB_ENV
        sleep 10

    - name: Wait for API readiness
      run: |
        cd dataframe-api
        chmod +x test.sh
        export API_BASE=http://localhost:4999
        ./test.sh wait

    - name: Run legacy core tests
      run: |
        cd dataframe-api
        export API_BASE=http://localhost:4999
        ./test.sh select
        ./test.sh filter
        ./test.sh groupby
        ./test.sh merge

    - name: Cleanup
      if: always()
      run: |
        if [ ! -z "$APP_PID" ]; then
          kill $APP_PID || true
        fi

  # Full test with Spark (optional, may be flaky in CI)
  test-with-spark:
    runs-on: ubuntu-latest
    continue-on-error: true  # Allow this job to fail without failing the workflow
    needs: [test-api-core]

    services:
      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.12'

    - name: Install dependencies
      run: |
        cd dataframe-api
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Create Docker network
      run: docker network create spark-net

    - name: Start Spark Master
      run: |
        docker run -d --name spark-master --network spark-net \
          -p 7077:7077 -p 8081:8081 \
          -e SPARK_MODE=master \
          -e SPARK_RPC_AUTHENTICATION_ENABLED=no \
          -e SPARK_RPC_ENCRYPTION_ENABLED=no \
          -e SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no \
          -e SPARK_SSL_ENABLED=no \
          -e SPARK_MASTER_HOST=0.0.0.0 \
          -e PYSPARK_PYTHON=python3 \
          bitnami/spark:3.5

    - name: Start Spark Worker
      run: |
        sleep 10  # Wait for master to start
        docker run -d --name spark-worker --network spark-net \
          -p 8082:8082 \
          -e SPARK_MODE=worker \
          -e SPARK_MASTER_URL=spark://spark-master:7077 \
          -e SPARK_WORKER_WEBUI_PORT=8082 \
          -e PYSPARK_PYTHON=python3 \
          bitnami/spark:3.5

    - name: Wait for Spark services
      run: |
        # Wait for Spark Master
        for i in {1..30}; do
          if curl -f http://localhost:8081 > /dev/null 2>&1; then
            echo "Spark Master is ready"
            break
          fi
          echo "Waiting for Spark Master... ($i/30)"
          sleep 10
        done
        
        # Wait for Spark Worker
        for i in {1..30}; do
          if curl -f http://localhost:8082 > /dev/null 2>&1; then
            echo "Spark Worker is ready"
            break
          fi
          echo "Waiting for Spark Worker... ($i/30)"
          sleep 10
        done

    - name: Start dataframe-api service with Spark
      run: |
        cd dataframe-api
        export FLASK_ENV=development
        export REDIS_HOST=localhost
        export REDIS_PORT=6379
        export PORT=5000
        export SPARK_MASTER_URL=spark://localhost:7077
        export SPARK_DRIVER_HOST=localhost
        export SPARK_DRIVER_BIND_ADDRESS=0.0.0.0
        export SPARK_EXECUTOR_MEMORY=2g
        export SPARK_DRIVER_MEMORY=2g
        export PYSPARK_PYTHON=python3
        export PYSPARK_DRIVER_PYTHON=python3
        export ENABLE_API_PROTECTION=false
        python app.py &
        SPARK_APP_PID=$!
        echo "SPARK_APP_PID=$SPARK_APP_PID" >> $GITHUB_ENV
        sleep 15

    - name: Wait for API readiness (Spark)
      run: |
        cd dataframe-api
        export API_BASE=http://localhost:5000
        ./test.sh wait

    - name: Run Spark-enabled tests
      run: |
        cd dataframe-api
        export API_BASE=http://localhost:5000
        ./test.sh compare-identical
        ./test.sh compare-schema

    - name: Cleanup Spark
      if: always()
      run: |
        if [ ! -z "$SPARK_APP_PID" ]; then
          kill $SPARK_APP_PID || true
        fi
        docker stop spark-worker spark-master || true
        docker rm spark-worker spark-master || true
        docker network rm spark-net || true