name: Tests

on:
  push:
    branches: [ main, master, develop ]
  pull_request:
    branches: [ main, master, develop ]
  workflow_dispatch:

jobs:
  # Core API tests without Spark (faster and more reliable)
  test-core:
    runs-on: ubuntu-latest

    services:
      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.12'

    - name: Install dependencies
      run: |
        cd dataframe-ui
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Wait for Redis
      run: |
        # Simple approach using netcat (already available on GitHub runners)
        until timeout 2 bash -c "</dev/tcp/localhost/6379"; do
          echo "Waiting for Redis..."
          sleep 2
        done
        echo "Redis is ready!"

    - name: Start dataframe-ui service (without Spark)
      run: |
        cd dataframe-ui
        export FLASK_ENV=development
        export REDIS_HOST=localhost
        export REDIS_PORT=6379
        export PORT=4999
        # Disable Spark for core tests - API will fallback to pandas
        export SPARK_MASTER_URL=""
        python app.py &
        APP_PID=$!
        echo "APP_PID=$APP_PID" >> $GITHUB_ENV
        
        # Wait for the app to start
        sleep 10

    - name: Wait for API readiness
      run: |
        cd dataframe-ui
        chmod +x test.sh
        export API_BASE=http://localhost:4999
        ./test.sh wait

    - name: Run all core tests (matching run_all function)
      run: |
        cd dataframe-ui
        export API_BASE=http://localhost:4999
        
        # Run each test that's part of run_all() function
        echo "Running test_select..."
        ./test.sh select
        
        echo "Running test_select_exclude..."
        ./test.sh select-exclude
        
        echo "Running test_groupby..."
        ./test.sh groupby
        
        echo "Running test_filter..."
        ./test.sh filter
        
        echo "Running test_merge..."
        ./test.sh merge
        
        echo "Running test_pivot..."
        ./test.sh pivot
        
        echo "Running test_compare_identical..."
        ./test.sh compare-identical
        
        echo "Running test_compare_schema..."
        ./test.sh compare-schema
        
        echo "Running test_mutate_total_value..."
        ./test.sh mutate
        
        echo "Running test_datetime_parse..."
        ./test.sh datetime
        
        echo "Running test_rename_dataframe..."
        ./test.sh rename

    - name: Run complete test suite
      run: |
        cd dataframe-ui
        export API_BASE=http://localhost:4999
        ./test.sh all

    - name: Cleanup
      if: always()
      run: |
        if [ ! -z "$APP_PID" ]; then
          kill $APP_PID || true
        fi

  # Full test with Spark (optional, may be flaky in CI)
  test-with-spark:
    runs-on: ubuntu-latest
    continue-on-error: true  # Allow this job to fail without failing the workflow

    services:
      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.12'

    - name: Install dependencies
      run: |
        cd dataframe-ui
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Create Docker network
      run: docker network create spark-net

    - name: Start Spark Master
      run: |
        docker run -d --name spark-master --network spark-net \
          -p 7077:7077 -p 8081:8081 \
          -e SPARK_MODE=master \
          -e SPARK_RPC_AUTHENTICATION_ENABLED=no \
          -e SPARK_RPC_ENCRYPTION_ENABLED=no \
          -e SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no \
          -e SPARK_SSL_ENABLED=no \
          -e SPARK_MASTER_HOST=0.0.0.0 \
          -e PYSPARK_PYTHON=python3 \
          bitnami/spark:3.5

    - name: Start Spark Worker
      run: |
        sleep 10  # Wait for master to start
        docker run -d --name spark-worker --network spark-net \
          -p 8082:8082 \
          -e SPARK_MODE=worker \
          -e SPARK_MASTER_URL=spark://spark-master:7077 \
          -e SPARK_WORKER_WEBUI_PORT=8082 \
          -e PYSPARK_PYTHON=python3 \
          bitnami/spark:3.5

    - name: Wait for Spark services
      run: |
        # Wait for Spark Master
        for i in {1..30}; do
          if curl -f http://localhost:8081 > /dev/null 2>&1; then
            echo "Spark Master is ready"
            break
          fi
          echo "Waiting for Spark Master... ($i/30)"
          sleep 10
        done
        
        # Wait for Spark Worker
        for i in {1..30}; do
          if curl -f http://localhost:8082 > /dev/null 2>&1; then
            echo "Spark Worker is ready"
            break
          fi
          echo "Waiting for Spark Worker... ($i/30)"
          sleep 10
        done

    - name: Start dataframe-ui service with Spark
      run: |
        cd dataframe-ui
        export FLASK_ENV=development
        export REDIS_HOST=localhost
        export REDIS_PORT=6379
        export PORT=5000
        export SPARK_MASTER_URL=spark://localhost:7077
        export SPARK_DRIVER_HOST=localhost
        export SPARK_DRIVER_BIND_ADDRESS=0.0.0.0
        export SPARK_EXECUTOR_MEMORY=2g
        export SPARK_DRIVER_MEMORY=2g
        export PYSPARK_PYTHON=python3
        export PYSPARK_DRIVER_PYTHON=python3
        python app.py &
        SPARK_APP_PID=$!
        echo "SPARK_APP_PID=$SPARK_APP_PID" >> $GITHUB_ENV
        
        # Wait for the app to start
        sleep 15

    - name: Wait for API readiness (Spark)
      run: |
        cd dataframe-ui
        export API_BASE=http://localhost:5000
        ./test.sh wait

    - name: Run Spark-enabled tests
      run: |
        cd dataframe-ui
        export API_BASE=http://localhost:5000
        # Run compare tests which benefit from Spark
        ./test.sh compare-identical
        ./test.sh compare-schema

    - name: Cleanup Spark
      if: always()
      run: |
        if [ ! -z "$SPARK_APP_PID" ]; then
          kill $SPARK_APP_PID || true
        fi
        docker stop spark-worker spark-master || true
        docker rm spark-worker spark-master || true
        docker network rm spark-net || true